#### TODO:

- Implement Polynomial Regression from scratch
- Implement `GD Variants` in Python

#### Gradient Descent

- a  minimization optimization algorithm (aka SGD - Stochastic Gradient Descent)
- how will the code change for `maximisation` problem? `Gradient Ascent` code ? -- > change the Params update rules , is there anything more to it ?
- how can I implement `Gradient Descent with Momentum` , `Gradient Descent with Adaptive Gradients` or adding an `Adaptive Learning Rate  in the base GD algorithm ?`

#### NLP 

-- todo: 

1. Text Preprocessing methods : `Tokenization, Stemming and Lemmatization, Stopwords Removal, POS Tagging, NER`
2. Feature Extraction : `Bag of Words, Tf-IDF, Word Embeddings`
3. Language Models /Deep Learning: `minimal version of BERT or any other language model, Neural network based language model to predict next word or token in the sequence, RNN, Attention mechanism implementation, LSTM`
4. Classical ML with NLP  : `Naive Bayes Clf, SVM`
5. Others : `Topic Modelling, Dependency Parsing, Sentiment Analysis, Wordcloud, Knowledge Graph from Natural Language, Text Summarization( both abstractive and extractive)`
